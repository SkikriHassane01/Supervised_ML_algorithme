{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hassaneskikri/logistic-regression-in-machine-learning?scriptVersionId=161482676\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"12951b85","metadata":{"papermill":{"duration":0.008632,"end_time":"2024-02-03T00:43:16.101502","exception":false,"start_time":"2024-02-03T00:43:16.09287","status":"completed"},"tags":[]},"source":["\n","\n","<div style =\"font-family:Trebuchet MS; background-color : #f8f0fa; border-left: 5px solid #1b4332; padding: 12px\">\n","    <h2 style=\"color: #1b4332; font-size: 48px; text-align: center;\"><b> Logistic regression in Machine Learning </b></h2>\n","    <hr style=\"border-top: 2px solid #264653;\">\n","    <h3 style=\"font-size: 14px; color: #264653; text-align: left; \"><strong>let's descover How does Logistic regression work and how can we implement it ? üòäüåü</strong></h3>\n","    <h2></h2>\n","</div>"]},{"cell_type":"markdown","id":"ffe36441","metadata":{"papermill":{"duration":0.007795,"end_time":"2024-02-03T00:43:16.117412","exception":false,"start_time":"2024-02-03T00:43:16.109617","status":"completed"},"tags":[]},"source":["<img src=\"https://th.bing.com/th/id/R.6e539a7757b473957c12c4f1d377215a?rik=3JVVjaRvTsRHJw&pid=ImgRaw&r=0\" alt=\"m\" width=\"1000\" height=\"600\">\n"]},{"cell_type":"markdown","id":"805595bf","metadata":{"papermill":{"duration":0.007697,"end_time":"2024-02-03T00:43:16.133181","exception":false,"start_time":"2024-02-03T00:43:16.125484","status":"completed"},"tags":[]},"source":["\n","<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîçüìö Outline üìöüîç\n","</h1>\n","\n","\n","\n","<ul style=\"font-family: 'Trebuchet MS', sans-serif; font-size: 120%; padding: 20px;\">\n","    <li>üåü Introduction to Logistic regression üåü</li>\n","    <li>üî¢ Type of logistic regression üî¢</li>\n","    <li>üöÄ The difference between linear and logistic regression. üöÄ</li>\n","    <li>üìö What is Segmoid function? üìö</li>\n","    <li>üìö cost Function üìö</li>\n","    <li>üî¢ Gradient descent üî¢</li>\n","    <li>üõ†Ô∏è Implementation of the Logistic regression Regression from scratchüõ†Ô∏è</li>\n","    <li>üõ†Ô∏è Implementation of the Logistic regression Regression using Scikit learnüõ†Ô∏è</li>\n","    <li>üéØ Conclusion üéØ</li>\n","</ul>\n"]},{"cell_type":"markdown","id":"049dcac9","metadata":{"papermill":{"duration":0.007591,"end_time":"2024-02-03T00:43:16.148764","exception":false,"start_time":"2024-02-03T00:43:16.141173","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüåü Introduction to Logistic Regression üåüüîª\n","</h1>\n"]},{"cell_type":"markdown","id":"693b3829","metadata":{"papermill":{"duration":0.007669,"end_time":"2024-02-03T00:43:16.164356","exception":false,"start_time":"2024-02-03T00:43:16.156687","status":"completed"},"tags":[]},"source":["Logistic regression is a technique in supervised machine learning used primarily for binary classification, which means it helps predict whether an instance belongs to one class or another based on certain features. \n","\n","\n","For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 it belongs to Class 0. It‚Äôs referred to as regression because it is the extension of linear regression but is mainly used for classification problems."]},{"cell_type":"markdown","id":"56b460f0","metadata":{"papermill":{"duration":0.007584,"end_time":"2024-02-03T00:43:16.179798","exception":false,"start_time":"2024-02-03T00:43:16.172214","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüî¢ Type of logistic regression üî¢üîª\n","</h1>\n"]},{"cell_type":"markdown","id":"d5b43029","metadata":{"papermill":{"duration":0.007571,"end_time":"2024-02-03T00:43:16.195259","exception":false,"start_time":"2024-02-03T00:43:16.187688","status":"completed"},"tags":[]},"source":["Logistic Regression can be classified into three types:\n","\n","- `Binomial:` There can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc.\n","- `Multinomial:` There can be 3 or more possible unordered types of the dependent variable, such as ‚Äúcat‚Äù, ‚Äúdogs‚Äù, or ‚Äúsheep‚Äù\n","- `Ordinal:` There can be 3 or more possible ordered types of dependent variables, such as ‚Äúlow‚Äù, ‚ÄúMedium‚Äù, or ‚ÄúHigh‚Äù."]},{"cell_type":"markdown","id":"0e026e38","metadata":{"papermill":{"duration":0.007714,"end_time":"2024-02-03T00:43:16.210913","exception":false,"start_time":"2024-02-03T00:43:16.203199","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªLinear Vs logistic regression üîª\n","</h1>\n"]},{"cell_type":"markdown","id":"cea47261","metadata":{"papermill":{"duration":0.007573,"end_time":"2024-02-03T00:43:16.226355","exception":false,"start_time":"2024-02-03T00:43:16.218782","status":"completed"},"tags":[]},"source":["## ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg)\n","\n","\n","| Feature                  | Linear Regression                                      | Logistic Regression                                      |\n","|--------------------------|--------------------------------------------------------|----------------------------------------------------------|\n","| **Definition**           | Used to predict the continuous dependent variable using| Used to predict the categorical dependent variable using |\n","|                          | a given set of independent variables.                   | a given set of independent variables.                     |\n","| **Output**               | Continuous values (e.g., sales, temperatures).         | Discrete values (e.g., 0 or 1, Yes or No).               |\n","| **Equation Form**        | $(y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_nx_n)$     | $(\\sigma(y) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_nx_n)}})$ |\n","| **Function Type**        | Linear function.                                       | Sigmoid (logistic) function.                             |\n","| **Usage**                | To predict the value of a variable based on the value  | To estimate the probability that a given input point     |\n","|                          | of other variables.                                    | belongs to a certain class.                              |\n","| **Example Applications** | - Predicting house prices based on features like size  | - Predicting whether an email is spam or not based on    |\n","|                          | and location.                                          | features like word frequencies.                          |\n","|                          | - Estimating the relationship between advertising      | - Diagnosing diseases based on patient test results.     |\n","|                          | budgets and sales.                                     |                                                          |\n"]},{"cell_type":"markdown","id":"e4b045ce","metadata":{"papermill":{"duration":0.008508,"end_time":"2024-02-03T00:43:16.243548","exception":false,"start_time":"2024-02-03T00:43:16.23504","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüìö What is segmoid function üìöüîª\n","</h1>\n"]},{"cell_type":"markdown","id":"5b20e04e","metadata":{"papermill":{"duration":0.007618,"end_time":"2024-02-03T00:43:16.259017","exception":false,"start_time":"2024-02-03T00:43:16.251399","status":"completed"},"tags":[]},"source":["First, logistic regression model transforms the linear regression function continuous value output into categorical value output using a sigmoid function, which maps any real-valued set of independent variables input into a value between 0 and 1. This function is known as the logistic function.\n","\n","So we will use the sigmoid function where the input will be z and we find the probability between 0 and 1. i.e. predicted y.\n","\n","\n","### segmoid formula\n","\n","$$\n","\\begin{array}{|l|}\n","\\hline\n","f(z) = \\frac{1}{1 + e^{-z}} \\\\\n","\\hline\n","\\end{array}\n","$$\n","\n","- As $(z)$ approaches negative infinity, $(e^{-z})$ approaches 0, and the sigmoid output approaches 0. \n","- As $(z)$ approaches positive infinity, $(e^{-z})$ approaches infinity, and the sigmoid output approaches 1. \n","\n","\n"]},{"cell_type":"markdown","id":"3b5d254d","metadata":{"papermill":{"duration":0.008021,"end_time":"2024-02-03T00:43:16.276237","exception":false,"start_time":"2024-02-03T00:43:16.268216","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüõ†Ô∏è Logistic Regression Equationüõ†Ô∏èüîª\n","</h1>\n"]},{"cell_type":"markdown","id":"5eaf5250","metadata":{"papermill":{"duration":0.00867,"end_time":"2024-02-03T00:43:16.292789","exception":false,"start_time":"2024-02-03T00:43:16.284119","status":"completed"},"tags":[]},"source":["Note that In logistic regression, the probability of an instance belonging to a particular class (e.g., class '1') can be represented using the sigmoid function $(\\sigma)$ as follows:\n","\n","- Probability of being in class '1': $P(y=1)$ = $\\sigma(z)$\n","- Probability of not being in class '1' (thus being in class '0'): $P(y=0)$ = 1 - $\\sigma(z)$\n","\n","before diving into the logistic regression equation, let's discover what is the odd?\n","The odd is the ratio of something occurring to something not occurring. it is different from probability as the probability is the ratio of something occurring to everything that could possibly occur. so odd will be:\n","\n","$$\n","\\begin{array}{|l|}\n","\\hline\n","\\frac{p(x)}{1-p(x)} = e^z\\\\\n","\\hline\n","\\end{array}\n","$$\n","\n","Applying natural log on odd. then log odd will be:\n","\n","$$\n","\\begin{aligned}\n","\\log \\left[\\frac{p(x)}{1-p(x)} \\right] &= z  \\\\\n","\\log \\left[\\frac{p(x)}{1-p(x)} \\right] &= w\\cdot X +b \\\\\n","\\frac{p(x)}{1-p(x)}&= e^{w\\cdot X +b} \\\\\n","p(x) &=e^{w\\cdot X +b}\\cdot (1-p(x)) \\\\\n","p(x) &=e^{w\\cdot X +b}-e^{w\\cdot X +b}\\cdot p(x)) \\\\\n","p(x)+e^{w\\cdot X +b}\\cdot p(x))&=e^{w\\cdot X +b} \\\\\n","p(x)(1+e^{w\\cdot X +b}) &=e^{w\\cdot X +b} \\\\\n","p(x)&= \\frac{e^{w\\cdot X +b}}{1+e^{w\\cdot X +b}}\n","\\end{aligned}\n","$$\n","\n","then the final logistic regression equation will be:\n","\n","$$\n","p(X;b,w) = \\frac{e^{w\\cdot X +b}}{1+e^{w\\cdot X +b}} = \\frac{1}{1+e^{-w\\cdot X +b}}=f_{\\mathbf{w},b}(\\mathbf{x}) \n","$$\n","\n","\n","$f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w} \\cdot\\mathbf{x}+b)$ where function $g$ is the sigmoid function."]},{"cell_type":"markdown","id":"0e6d28a9","metadata":{"papermill":{"duration":0.007762,"end_time":"2024-02-03T00:43:16.308782","exception":false,"start_time":"2024-02-03T00:43:16.30102","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªCost Functionüîª\n","</h1>\n"]},{"cell_type":"markdown","id":"a69d1172","metadata":{"papermill":{"duration":0.007683,"end_time":"2024-02-03T00:43:16.324485","exception":false,"start_time":"2024-02-03T00:43:16.316802","status":"completed"},"tags":[]},"source":["First note that **Loss** is a measure of the difference of a single example to its target value while the and **Cost** is a measure of the losses over the training set\n","\n","\n","The loss is defined: \n","\n","\\begin{equation}\n","  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n","    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n","    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n","  \\end{cases}\n","\\end{equation}\n","\n","\n","*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n","\n","*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n","\n","\n","Let's simplify this :\n","\n","First when $ y^{(i)} = 0$, the left-hand term is eliminated:\n","$$\n","\\begin{align}\n","loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n","&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n","\\end{align}\n","$$\n","and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n","$$\n","\\begin{align}\n","  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n","  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n","\\end{align}\n","$$\n","\n","\n","\n","\n","and the cost function is of the forme:\n","\n","$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] $$\n","\n","\n","#### where:\n","\n","*  m is the number of training examples in the data set\n","* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n","\n","    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n","    \n","* and :\n","\n","$$\n","\\begin{align}\n","  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)}) \\\\\n","  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}  \\\\\n","  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b \\\\\n","\\end{align}\n","$$"]},{"cell_type":"markdown","id":"055d9a43","metadata":{"papermill":{"duration":0.007678,"end_time":"2024-02-03T00:43:16.340196","exception":false,"start_time":"2024-02-03T00:43:16.332518","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüî¢ Gradient descent üî¢üîª\n","</h1>\n"]},{"cell_type":"markdown","id":"f298459a","metadata":{"papermill":{"duration":0.007851,"end_time":"2024-02-03T00:43:16.356175","exception":false,"start_time":"2024-02-03T00:43:16.348324","status":"completed"},"tags":[]},"source":["$$\\begin{align*}\n","&\\text{repeat until convergence:} \\; \\lbrace \\\\\n","&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}     \\; & \\text{for j := 0..n-1} \\\\ \n","&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n","&\\rbrace\n","\\end{align*}$$\n","\n","Where each iteration performs simultaneous updates on $w_j$ for all $j$, where\n","$$\\begin{align*}\n","\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}     \\\\\n","\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})   \n","\\end{align*}$$\n","\n","* m is the number of training examples in the data set      \n","* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n","* For a logistic regression model  \n","    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n","    $f_{\\mathbf{w},b}(x) = g(z)$  \n","    where $g(z)$ is the sigmoid function:  \n","    $g(z) = \\frac{1}{1+e^{-z}}$   \n","    "]},{"cell_type":"markdown","id":"36764c2e","metadata":{"papermill":{"duration":0.007652,"end_time":"2024-02-03T00:43:16.371893","exception":false,"start_time":"2024-02-03T00:43:16.364241","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüõ†Ô∏è Decision Boundaryüõ†Ô∏èüîª\n","</h1>\n"]},{"cell_type":"markdown","id":"5b1477af","metadata":{"papermill":{"duration":0.007827,"end_time":"2024-02-03T00:43:16.387509","exception":false,"start_time":"2024-02-03T00:43:16.379682","status":"completed"},"tags":[]},"source":["Once the model is trained and the optimal weights are found, we can use the Hypothesis Function to make predictions. Since the Hypothesis Function outputs the probability of an instance belonging to the positive class, we need to set a threshold (usually 0.5) to classify the instance as either positive or negative. The decision boundary is the line or hyperplane that separates the two classes in the feature space.\n","\n","\n","![](https://miro.medium.com/v2/resize:fit:750/format:webp/0*dOPIXKyly5h254Yx.png)"]},{"cell_type":"markdown","id":"08b38c1e","metadata":{"papermill":{"duration":0.007677,"end_time":"2024-02-03T00:43:16.406979","exception":false,"start_time":"2024-02-03T00:43:16.399302","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüõ†Ô∏è Implementation of the Logistic regression Regression from scratchüõ†Ô∏èüîª\n","</h1>\n"]},{"cell_type":"code","execution_count":1,"id":"7abcfb66","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:16.424755Z","iopub.status.busy":"2024-02-03T00:43:16.424071Z","iopub.status.idle":"2024-02-03T00:43:18.961134Z","shell.execute_reply":"2024-02-03T00:43:18.960132Z"},"papermill":{"duration":2.549011,"end_time":"2024-02-03T00:43:18.963861","exception":false,"start_time":"2024-02-03T00:43:16.41485","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"id":"f398197c","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:18.981821Z","iopub.status.busy":"2024-02-03T00:43:18.981281Z","iopub.status.idle":"2024-02-03T00:43:19.020531Z","shell.execute_reply":"2024-02-03T00:43:19.019376Z"},"papermill":{"duration":0.051399,"end_time":"2024-02-03T00:43:19.023511","exception":false,"start_time":"2024-02-03T00:43:18.972112","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["diabete = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n","diabete.head()"]},{"cell_type":"code","execution_count":3,"id":"439027bb","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:19.042374Z","iopub.status.busy":"2024-02-03T00:43:19.041958Z","iopub.status.idle":"2024-02-03T00:43:19.051689Z","shell.execute_reply":"2024-02-03T00:43:19.050583Z"},"papermill":{"duration":0.022095,"end_time":"2024-02-03T00:43:19.054214","exception":false,"start_time":"2024-02-03T00:43:19.032119","status":"completed"},"tags":[]},"outputs":[],"source":["X = diabete.iloc[:, :-1]\n","y=diabete.iloc[:,-1]"]},{"cell_type":"code","execution_count":4,"id":"772a4a4b","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:19.073421Z","iopub.status.busy":"2024-02-03T00:43:19.072886Z","iopub.status.idle":"2024-02-03T00:43:19.086666Z","shell.execute_reply":"2024-02-03T00:43:19.085627Z"},"papermill":{"duration":0.026243,"end_time":"2024-02-03T00:43:19.088996","exception":false,"start_time":"2024-02-03T00:43:19.062753","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape: (614, 8)\n","y_train shape: (614,)\n","X_test shape: (154, 8)\n","y_test shape: (154,)\n"]}],"source":["X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")\n"]},{"cell_type":"code","execution_count":5,"id":"3f15e835","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:19.108121Z","iopub.status.busy":"2024-02-03T00:43:19.107293Z","iopub.status.idle":"2024-02-03T00:43:34.456335Z","shell.execute_reply":"2024-02-03T00:43:34.455308Z"},"papermill":{"duration":15.36219,"end_time":"2024-02-03T00:43:34.459675","exception":false,"start_time":"2024-02-03T00:43:19.097485","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss after iteration 0: 0.6931471805599454\n","Loss after iteration 1000: 0.6205228877606628\n","Loss after iteration 2000: 0.6189435335797971\n","Loss after iteration 3000: 0.617806063826578\n","Loss after iteration 4000: 0.6169442643252923\n","Loss after iteration 5000: 0.6162804782457397\n","Loss after iteration 6000: 0.6157596511144128\n","Loss after iteration 7000: 0.6153421695652554\n","Loss after iteration 8000: 0.6149994832422139\n","Loss after iteration 9000: 0.6147109958230045\n","Training data Accuracy: 67.26384364820846%\n","Testing data Accuracy: 72.07792207792207%\n"]}],"source":["# Step by step until we are done:\n","\n","# Remember to understand the theoretical part in the above section before implementing it.\n","# Also, do your trial and then you can see the provided code here;\n","\n","# first let's Initialize the w and b parameter:\n","weights = np.zeros(X.shape[1])  #w\n","bias = 0                        #b\n","\n","\n","# Sigmoid function or logistic function:\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","# Loss function:\n","def compute_loss(y, y_hat):  # y_hat:the prediction\n","    m = y.shape[0]\n","    return -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n","\n","# Gradient Descent function to update weights and bias \n","\n","# Note that we must update them simultanious:\n","def gradient_descent(X, y, weights, bias, learning_rate, iterations):\n","    m = X.shape[0]                       # m is the number of training examples in the data set  \n","    \n","    for i in range(iterations):\n","        \n","        # Compute the prediction\n","        z = np.dot(X, weights) + bias\n","        y_hat = sigmoid(z)\n","\n","        #Compute gradients\n","        dw = 1/m * np.dot(X.T, (y_hat - y))\n","        db = 1/m * np.sum(y_hat - y)\n","\n","        # Update parameters\n","        weights -= learning_rate * dw\n","        bias -= learning_rate * db\n","\n","        # Compute and print loss\n","        loss = compute_loss(y, y_hat)\n","        if i%1000 == 0:\n","            print(f\"Loss after iteration {i}: {loss}\")\n","\n","    return weights, bias\n","\n","# gradient descent parameters note that you can chage them if you want \n","learning_rate = 0.0001 \n","iterations = 10000\n","\n","# Train the model\n","weights, bias = gradient_descent(X_train, y_train, weights, bias, learning_rate, iterations)\n","\n","# Predict function\n","def predict(X, weights, bias):\n","    z = np.dot(X, weights) + bias\n","    y_pred = sigmoid(z)\n","    return y_pred.round()\n","\n","# training and testing data prediction\n","train_predictions = predict(X_train, weights, bias)\n","test_predictions= predict(X_test,weights,bias)\n","\n","# Check the accuracy \n","train_accuracy = np.mean(train_predictions == y_train)\n","test_accuracy = np.mean(test_predictions == y_test)\n","\n","print(f\"Training data Accuracy: {train_accuracy * 100}%\")\n","print(f\"Testing data Accuracy: {test_accuracy * 100}%\")\n","\n"]},{"cell_type":"markdown","id":"d2e5ad5d","metadata":{"papermill":{"duration":0.009114,"end_time":"2024-02-03T00:43:34.478778","exception":false,"start_time":"2024-02-03T00:43:34.469664","status":"completed"},"tags":[]},"source":["<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üîªüõ†Ô∏è Implementation of the Logistic regression Regression using Scikit learnüõ†Ô∏èüîª\n","</h1>\n"]},{"cell_type":"code","execution_count":6,"id":"834c2f6d","metadata":{"execution":{"iopub.execute_input":"2024-02-03T00:43:34.499146Z","iopub.status.busy":"2024-02-03T00:43:34.498732Z","iopub.status.idle":"2024-02-03T00:43:34.636263Z","shell.execute_reply":"2024-02-03T00:43:34.63482Z"},"papermill":{"duration":0.150586,"end_time":"2024-02-03T00:43:34.638609","exception":false,"start_time":"2024-02-03T00:43:34.488023","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression accuracy on training data using scikit-learn: 77.20%\n","Logistic Regression accuracy on test data using scikit-learn: 74.68%\n","Custom Logistic Regression accuracy on training data: 67.26%\n","Custom Logistic Regression accuracy on test data: 72.08%\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","reg = LogisticRegression()\n","\n","# Fit the model on the training data\n","reg.fit(X_train, y_train)\n","\n","train_pred = reg.predict(X_train)\n","test_pred = reg.predict(X_test)\n","\n","# Calculate accuracy using the scikit-learn model\n","train_accuracy_scik = accuracy_score(y_train, train_pred)\n","test_accuracy_scik = accuracy_score(y_test, test_pred)\n","\n","print(f\"Logistic Regression accuracy on training data using scikit-learn: {train_accuracy_scik * 100:.2f}%\")\n","print(f\"Logistic Regression accuracy on test data using scikit-learn: {test_accuracy_scik * 100:.2f}%\")\n","\n","# Now compare with our custom model\n","\n","print(f\"Custom Logistic Regression accuracy on training data: {train_accuracy * 100:.2f}%\")\n","print(f\"Custom Logistic Regression accuracy on test data: {test_accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","id":"ed75fdc9","metadata":{"papermill":{"duration":0.009191,"end_time":"2024-02-03T00:43:34.657517","exception":false,"start_time":"2024-02-03T00:43:34.648326","status":"completed"},"tags":[]},"source":["\n","<h1 style=\"background-color: #12f7ff; font-family: 'Trebuchet MS', sans-serif; color: #000; font-size: 150%; text-align: center; border-radius: 50px 15px; padding: 10px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n","    üéØ Conclusion üîç\n","</h1>\n"]},{"cell_type":"markdown","id":"47d9c022","metadata":{"papermill":{"duration":0.009066,"end_time":"2024-02-03T00:43:34.676043","exception":false,"start_time":"2024-02-03T00:43:34.666977","status":"completed"},"tags":[]},"source":["Logistic regression is a fundamental machine learning algorithm crucial for solving classification problems, where the goal is to categorize data into distinct classes. Its ability to provide probabilities alongside classifications makes it invaluable for decision-making processes in various fields, such as medicine, finance, and social sciences.\n","\n","\n","Finally, if you found this article useful, you can check out my repository on GitHub. It covers all the skills you need to become a data scientist. roadmap [here](https://github.com/HassaneSkikri/Step_by_Step_to_Learn_Data_Science)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":228,"sourceId":482,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":21.995946,"end_time":"2024-02-03T00:43:35.205614","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-03T00:43:13.209668","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}